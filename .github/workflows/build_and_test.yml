#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

name: Build and test

on:
  workflow_call:
    inputs:
      java:
        required: false
        type: string
        default: 17
      branch:
        description: Branch to run the build against
        required: false
        type: string
        # Change 'master' to 'branch-4.0' in branch-4.0 branch after cutting it.
        default: master
      hadoop:
        description: Hadoop version to run with. HADOOP_PROFILE environment variable should accept it.
        required: false
        type: string
        default: hadoop3
      envs:
        description: Additional environment variables to set when running the tests. Should be in JSON format.
        required: false
        type: string
        default: "{}"
      jobs:
        description: >-
          Jobs to run, and should be in JSON format. The values should be matched with the job's key defined
          in this file, e.g., build. See precondition job below.
        required: false
        type: string
        default: '{"tpcds-1g":"true"}'
jobs:
  # Any TPC-DS related updates on this job need to be applied to tpcds-1g-gen job of benchmark.yml as well
  tpcds-1g:
    name: Run TPC-DS queries with SF=1
    # Pin to 'Ubuntu 20.04' due to 'databricks/tpcds-kit' compilation
    runs-on: ubuntu-20.04
    timeout-minutes: 180
    env:
      SPARK_LOCAL_IP: localhost
    steps:
      - name: Checkout Spark repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          repository: apache/spark
          ref: ${{ inputs.branch }}
      - name: Cache SBT and Maven
        uses: actions/cache@v4
        with:
          path: |
            build/apache-maven-*
            build/*.jar
            ~/.sbt
          key: build-${{ hashFiles('**/pom.xml', 'project/build.properties', 'build/mvn', 'build/sbt', 'build/sbt-launch-lib.bash', 'build/spark-build-info') }}
          restore-keys: |
            build-
      - name: Cache Coursier local repository
        uses: actions/cache@v4
        with:
          path: ~/.cache/coursier
          key: tpcds-coursier-${{ hashFiles('**/pom.xml', '**/plugins.sbt') }}
          restore-keys: |
            tpcds-coursier-
      - name: Install Java ${{ inputs.java }}
        uses: actions/setup-java@v4
        with:
          distribution: zulu
          java-version: ${{ inputs.java }}
      - name: Cache TPC-DS generated data
        id: cache-tpcds-sf-1
        uses: actions/cache@v4
        with:
          path: ./tpcds-sf-1
          key: tpcds-${{ hashFiles('.github/workflows/build_and_test.yml', 'sql/core/src/test/scala/org/apache/spark/sql/TPCDSSchema.scala') }}
      - name: Checkout tpcds-kit repository
        if: steps.cache-tpcds-sf-1.outputs.cache-hit != 'true'
        uses: actions/checkout@v4
        with:
          repository: databricks/tpcds-kit
          ref: 2a5078a782192ddb6efbcead8de9973d6ab4f069
          path: ./tpcds-kit
      - name: Build tpcds-kit
        if: steps.cache-tpcds-sf-1.outputs.cache-hit != 'true'
        run: cd tpcds-kit/tools && make OS=LINUX
      - name: Generate TPC-DS (SF=1) table data
        if: steps.cache-tpcds-sf-1.outputs.cache-hit != 'true'
        run: build/sbt "sql/Test/runMain org.apache.spark.sql.GenTPCDSData --dsdgenDir `pwd`/tpcds-kit/tools --location `pwd`/tpcds-sf-1 --scaleFactor 1 --numPartitions 1 --overwrite"
      - name: Run TPC-DS queries (Sort merge join)
        run: |
          SPARK_TPCDS_DATA=`pwd`/tpcds-sf-1 build/sbt "sql/testOnly org.apache.spark.sql.TPCDSQueryTestSuite"
        env:
          SPARK_ANSI_SQL_MODE: ${{ fromJSON(inputs.envs).SPARK_ANSI_SQL_MODE }}
          SPARK_TPCDS_JOIN_CONF: |
            spark.sql.autoBroadcastJoinThreshold=-1
            spark.sql.join.preferSortMergeJoin=true
      - name: Run TPC-DS queries (Broadcast hash join)
        run: |
          SPARK_TPCDS_DATA=`pwd`/tpcds-sf-1 build/sbt "sql/testOnly org.apache.spark.sql.TPCDSQueryTestSuite"
        env:
          SPARK_ANSI_SQL_MODE: ${{ fromJSON(inputs.envs).SPARK_ANSI_SQL_MODE }}
          SPARK_TPCDS_JOIN_CONF: |
            spark.sql.autoBroadcastJoinThreshold=10485760
      - name: Run TPC-DS queries (Shuffled hash join)
        run: |
          SPARK_TPCDS_DATA=`pwd`/tpcds-sf-1 build/sbt "sql/testOnly org.apache.spark.sql.TPCDSQueryTestSuite"
        env:
          SPARK_ANSI_SQL_MODE: ${{ fromJSON(inputs.envs).SPARK_ANSI_SQL_MODE }}
          SPARK_TPCDS_JOIN_CONF: |
            spark.sql.autoBroadcastJoinThreshold=-1
            spark.sql.join.forceApplyShuffledHashJoin=true
      - name: Run TPC-DS queries on collated data
        run: |
          SPARK_TPCDS_DATA=`pwd`/tpcds-sf-1 build/sbt "sql/testOnly org.apache.spark.sql.TPCDSCollationQueryTestSuite"
      - name: Upload test results to report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-tpcds--${{ inputs.java }}-${{ inputs.hadoop }}-hive2.3
          path: "**/target/test-reports/*.xml"
      - name: Upload unit tests log files
        if: ${{ !success() }}
        uses: actions/upload-artifact@v4
        with:
          name: unit-tests-log-tpcds--${{ inputs.java }}-${{ inputs.hadoop }}-hive2.3
          path: "**/target/unit-tests.log"
