#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

name: Build and test

on:
  workflow_call:
    inputs:
      java:
        required: false
        type: string
        default: 17
      branch:
        description: Branch to run the build against
        required: false
        type: string
        # Change 'master' to 'branch-4.0' in branch-4.0 branch after cutting it.
        default: master
      hadoop:
        description: Hadoop version to run with. HADOOP_PROFILE environment variable should accept it.
        required: false
        type: string
        default: hadoop3
      envs:
        description: Additional environment variables to set when running the tests. Should be in JSON format.
        required: false
        type: string
        default: '{}'
      jobs:
        description: >-
          Jobs to run, and should be in JSON format. The values should be matched with the job's key defined
          in this file, e.g., build. See precondition job below.
        required: false
        type: string
        default: ''
jobs:
  precondition:
    name: Check changes
    runs-on: ubuntu-latest
    env:
      GITHUB_PREV_SHA: ${{ github.event.before }}
    outputs:
      required: ${{ steps.set-outputs.outputs.required }}
      image_url: ${{ steps.infra-image-outputs.outputs.image_url }}
    steps:
    - name: Checkout Spark repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        repository: apache/spark
        ref: ${{ inputs.branch }}
    - name: Sync the current branch with the latest in Apache Spark
      if: github.repository != 'apache/spark'
      run: |
        echo "APACHE_SPARK_REF=$(git rev-parse HEAD)" >> $GITHUB_ENV
        git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m "Merged commit" --allow-empty
    - name: Check all modules
      id: set-outputs
      run: |
        if [ -z "${{ inputs.jobs }}" ]; then
          pyspark_modules=`cd dev && python -c "import sparktestsupport.modules as m; print(','.join(m.name for m in m.all_modules if m.name.startswith('pyspark')))"`
          pyspark=`./dev/is-changed.py -m $pyspark_modules`
          if [[ "${{ github.repository }}" != 'apache/spark' ]]; then
            pandas=$pyspark
            yarn=`./dev/is-changed.py -m yarn`
            kubernetes=`./dev/is-changed.py -m kubernetes`
            sparkr=`./dev/is-changed.py -m sparkr`
            tpcds=`./dev/is-changed.py -m sql`
            docker=`./dev/is-changed.py -m docker-integration-tests`
            buf=true
            ui=true
            docs=true
          else
            pandas=false
            yarn=false
            kubernetes=false
            sparkr=false
            tpcds=false
            docker=false
            buf=false
            ui=false
            docs=false
          fi
          build=`./dev/is-changed.py -m "core,unsafe,kvstore,avro,utils,network-common,network-shuffle,repl,launcher,examples,sketch,variant,api,catalyst,hive-thriftserver,mllib-local,mllib,graphx,streaming,sql-kafka-0-10,streaming-kafka-0-10,streaming-kinesis-asl,kubernetes,hadoop-cloud,spark-ganglia-lgpl,protobuf,yarn,connect,sql,hive"`
          precondition="
            {
              \"build\": \"$build\",
              \"pyspark\": \"$pyspark\",
              \"pyspark-pandas\": \"$pandas\",
              \"sparkr\": \"$sparkr\",
              \"tpcds-1g\": \"$tpcds\",
              \"docker-integration-tests\": \"$docker\",
              \"lint\" : \"true\",
              \"docs\" : \"$docs\",
              \"yarn\" : \"$yarn\",
              \"k8s-integration-tests\" : \"$kubernetes\",
              \"buf\" : \"$buf\",
              \"ui\" : \"$ui\",
            }"
          echo $precondition # For debugging
          # Remove `\n` to avoid "Invalid format" error
          precondition="${precondition//$'\n'/}}"
          echo "required=$precondition" >> $GITHUB_OUTPUT
        else
          # This is usually set by scheduled jobs.
          precondition='${{ inputs.jobs }}'
          echo $precondition # For debugging
          precondition="${precondition//$'\n'/}"
          echo "required=$precondition" >> $GITHUB_OUTPUT
        fi
    - name: Generate infra image URL
      id: infra-image-outputs
      run: |
        # Convert to lowercase to meet Docker repo name requirement
        REPO_OWNER=$(echo "${{ github.repository_owner }}" | tr '[:upper:]' '[:lower:]')
        IMG_NAME="apache-spark-ci-image:${{ inputs.branch }}-${{ github.run_id }}"
        IMG_URL="ghcr.io/$REPO_OWNER/$IMG_NAME"
        echo "image_url=$IMG_URL" >> $GITHUB_OUTPUT

  sparkr-window:
    name: "Build Sparkr Window"
    runs-on: windows-2022
    timeout-minutes: 300
    steps:
      - name: Download winutils Hadoop binary
        uses: actions/checkout@v4
        with:
          repository: cdarlint/winutils
      - name: Move Hadoop winutil into home directory
        run: |
          Move-Item -Path hadoop-3.3.6 -Destination ~\
      - name: Checkout Spark repository
        uses: actions/checkout@v4
      - name: Cache Maven local repository
        uses: actions/cache@v4
        with:
          path: ~/.m2/repository
          key: build-sparkr-windows-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            build-sparkr-windows-maven-
      - name: Install Java 17
        uses: actions/setup-java@v4
        with:
          distribution: zulu
          java-version: 17
      - name: Install R 4.4.0
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: 4.4.0
      - name: Install R dependencies
        run: |
          Rscript -e "install.packages(c('knitr', 'rmarkdown', 'testthat', 'e1071', 'survival', 'arrow', 'xml2'), repos='https://cloud.r-project.org/')"
          Rscript -e "pkg_list <- as.data.frame(installed.packages()[,c(1, 3:4)]); pkg_list[is.na(pkg_list$Priority), 1:2, drop = FALSE]"
        shell: cmd
      # SparkR build does not need Python. However, it shows warnings when the Python version is too low during
      # the attempt to look up Python Data Sources for session initialization. The Windows 2019 runner
      # includes Python 3.7, which Spark does not support. Therefore, we simply install the proper Python
      # for simplicity, see SPARK-47116.
      - name: Install Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          architecture: x64
      - name: Build Spark
        run: |
          rem 1. '-Djna.nosys=true' is required to avoid kernel32.dll load failure.
          rem   See SPARK-28759.
          rem 2. Ideally we should check the tests related to Hive in SparkR as well (SPARK-31745).
          rem 3. setup-java installs Maven 3.8.7 but does not allow changing its version, so overwrite
          rem   Maven version as a workaround.
          mvn -DskipTests -Psparkr -Djna.nosys=true package -Dmaven.version=3.8.7
        shell: cmd
      - name: Run SparkR tests
        run: |
          set HADOOP_HOME=%USERPROFILE%\hadoop-3.3.6
          set PATH=%HADOOP_HOME%\bin;%PATH%
          .\bin\spark-submit2.cmd --driver-java-options "-Dlog4j.configurationFile=file:///%CD:\=/%/R/log4j2.properties" --conf spark.hadoop.fs.defaultFS="file:///" R\pkg\tests\run-all.R
        shell: cmd
        env:
          NOT_CRAN: true
          # See SPARK-27848. Currently installing some dependent packages causes
          # "(converted from warning) unable to identify current timezone 'C':" for an unknown reason.
          # This environment variable works around to test SparkR against a higher version.
          R_REMOTES_NO_ERRORS_FROM_WARNINGS: true
