#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

name: Build and test

on:
  workflow_call:
    inputs:
      java:
        required: false
        type: string
        default: 17
      branch:
        description: Branch to run the build against
        required: false
        type: string
        # Change 'master' to 'branch-4.0' in branch-4.0 branch after cutting it.
        default: master
      hadoop:
        description: Hadoop version to run with. HADOOP_PROFILE environment variable should accept it.
        required: false
        type: string
        default: hadoop3
      envs:
        description: Additional environment variables to set when running the tests. Should be in JSON format.
        required: false
        type: string
        default: "{}"
      jobs:
        description: >-
          Jobs to run, and should be in JSON format. The values should be matched with the job's key defined
          in this file, e.g., build. See precondition job below.
        required: false
        type: string
        default: "{'tpcds-1g':'true'}"
jobs:
  precondition:
    name: Check changes
    runs-on: ubuntu-latest
    env:
      GITHUB_PREV_SHA: ${{ github.event.before }}
    outputs:
      required: ${{ steps.set-outputs.outputs.required }}
      image_url: ${{ steps.infra-image-outputs.outputs.image_url }}
      image_docs_url: ${{ steps.infra-image-docs-outputs.outputs.image_docs_url }}
      image_docs_url_link: ${{ steps.infra-image-link.outputs.image_docs_url_link }}
      image_lint_url: ${{ steps.infra-image-lint-outputs.outputs.image_lint_url }}
      image_lint_url_link: ${{ steps.infra-image-link.outputs.image_lint_url_link }}
    steps:
      # 1.这里指定拉去apache/spark仓库的指定分支代码
      - name: Checkout Spark repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          repository: apache/spark
          ref: ${{ inputs.branch }}
      - name: Check all modules
        id: set-outputs
        run: |
          if [ -z "${{ inputs.jobs }}" ]; then
            pyspark_modules=`cd dev && python -c "import sparktestsupport.modules as m; print(','.join(m.name for m in m.all_modules if m.name.startswith('pyspark')))"`
            pyspark=`./dev/is-changed.py -m $pyspark_modules`
            if [[ "${{ github.repository }}" != 'apache/spark' ]]; then
              pandas=$pyspark
              yarn=`./dev/is-changed.py -m yarn`
              kubernetes=`./dev/is-changed.py -m kubernetes`
              sparkr=`./dev/is-changed.py -m sparkr`
              tpcds=`./dev/is-changed.py -m sql`
              docker=`./dev/is-changed.py -m docker-integration-tests`
              buf=true
              ui=true
              docs=true
            else
              pandas=false
              yarn=false
              kubernetes=false
              sparkr=false
              tpcds=false
              docker=false
              buf=false
              ui=false
              docs=false
            fi
            build=`./dev/is-changed.py -m "core,unsafe,kvstore,avro,utils,network-common,network-shuffle,repl,launcher,examples,sketch,variant,api,catalyst,hive-thriftserver,mllib-local,mllib,graphx,streaming,sql-kafka-0-10,streaming-kafka-0-10,streaming-kinesis-asl,kubernetes,hadoop-cloud,spark-ganglia-lgpl,protobuf,yarn,connect,sql,hive"`
            precondition="
              {
                \"build\": \"$build\",
                \"pyspark\": \"$pyspark\",
                \"pyspark-pandas\": \"$pandas\",
                \"sparkr\": \"$sparkr\",
                \"tpcds-1g\": \"$tpcds\",
                \"docker-integration-tests\": \"$docker\",
                \"lint\" : \"true\",
                \"docs\" : \"$docs\",
                \"yarn\" : \"$yarn\",
                \"k8s-integration-tests\" : \"$kubernetes\",
                \"buf\" : \"$buf\",
                \"ui\" : \"$ui\",
              }"
            echo $precondition # For debugging
            # Remove `\n` to avoid "Invalid format" error
            precondition="${precondition//$'\n'/}}"
            echo "required=$precondition" >> $GITHUB_OUTPUT
          else
            # This is usually set by scheduled jobs.
            precondition='${{ inputs.jobs }}'
            echo $precondition # For debugging
            precondition="${precondition//$'\n'/}"
            echo "required=$precondition" >> $GITHUB_OUTPUT
          fi
      - name: Generate infra image URL
        id: infra-image-outputs
        run: |
          # Convert to lowercase to meet Docker repo name requirement
          REPO_OWNER=$(echo "${{ github.repository_owner }}" | tr '[:upper:]' '[:lower:]')
          IMG_NAME="apache-spark-ci-image:${{ inputs.branch }}-${{ github.run_id }}"
          IMG_URL="ghcr.io/$REPO_OWNER/$IMG_NAME"
          echo "image_url=$IMG_URL" >> $GITHUB_OUTPUT
      - name: Generate infra image URL (Documentation)
        id: infra-image-docs-outputs
        run: |
          # Convert to lowercase to meet Docker repo name requirement
          REPO_OWNER=$(echo "${{ github.repository_owner }}" | tr '[:upper:]' '[:lower:]')
          IMG_NAME="apache-spark-ci-image-docs:${{ inputs.branch }}-${{ github.run_id }}"
          IMG_URL="ghcr.io/$REPO_OWNER/$IMG_NAME"
          echo "image_docs_url=$IMG_URL" >> $GITHUB_OUTPUT
      - name: Generate infra image URL (Linter)
        id: infra-image-lint-outputs
        run: |
          # Convert to lowercase to meet Docker repo name requirement
          REPO_OWNER=$(echo "${{ github.repository_owner }}" | tr '[:upper:]' '[:lower:]')
          IMG_NAME="apache-spark-ci-image-lint:${{ inputs.branch }}-${{ github.run_id }}"
          IMG_URL="ghcr.io/$REPO_OWNER/$IMG_NAME"
          echo "image_lint_url=$IMG_URL" >> $GITHUB_OUTPUT
      - name: Link the docker images
        id: infra-image-link
        run: |
          # Set the image URL for job "docs"
          # Should delete the link and directly use image_docs_url after SPARK 3.x EOL
          if [[ "${{ inputs.branch }}" == 'branch-3.5' ]]; then
            echo "image_docs_url_link=${{ steps.infra-image-outputs.outputs.image_url }}" >> $GITHUB_OUTPUT
            echo "image_lint_url_link=${{ steps.infra-image-outputs.outputs.image_url }}" >> $GITHUB_OUTPUT
          else
            echo "image_docs_url_link=${{ steps.infra-image-docs-outputs.outputs.image_docs_url }}" >> $GITHUB_OUTPUT
            echo "image_lint_url_link=${{ steps.infra-image-lint-outputs.outputs.image_lint_url }}" >> $GITHUB_OUTPUT
          fi

  # Any TPC-DS related updates on this job need to be applied to tpcds-1g-gen job of benchmark.yml as well
  tpcds-1g:
    needs: precondition
    if: fromJson(needs.precondition.outputs.required).tpcds-1g == 'true'
    name: Run TPC-DS queries with SF=1
    # Pin to 'Ubuntu 20.04' due to 'databricks/tpcds-kit' compilation
    runs-on: ubuntu-20.04
    timeout-minutes: 180
    env:
      SPARK_LOCAL_IP: localhost
    steps:
      - name: Checkout Spark repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          repository: apache/spark
          ref: ${{ inputs.branch }}
      - name: Sync the current branch with the latest in Apache Spark
        if: github.repository != 'apache/spark'
        run: |
          git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}
          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD
          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m "Merged commit" --allow-empty
      - name: Cache SBT and Maven
        uses: actions/cache@v4
        with:
          path: |
            build/apache-maven-*
            build/*.jar
            ~/.sbt
          key: build-${{ hashFiles('**/pom.xml', 'project/build.properties', 'build/mvn', 'build/sbt', 'build/sbt-launch-lib.bash', 'build/spark-build-info') }}
          restore-keys: |
            build-
      - name: Cache Coursier local repository
        uses: actions/cache@v4
        with:
          path: ~/.cache/coursier
          key: tpcds-coursier-${{ hashFiles('**/pom.xml', '**/plugins.sbt') }}
          restore-keys: |
            tpcds-coursier-
      - name: Install Java ${{ inputs.java }}
        uses: actions/setup-java@v4
        with:
          distribution: zulu
          java-version: ${{ inputs.java }}
      - name: Cache TPC-DS generated data
        id: cache-tpcds-sf-1
        uses: actions/cache@v4
        with:
          path: ./tpcds-sf-1
          key: tpcds-${{ hashFiles('.github/workflows/build_and_test.yml', 'sql/core/src/test/scala/org/apache/spark/sql/TPCDSSchema.scala') }}
      - name: Checkout tpcds-kit repository
        if: steps.cache-tpcds-sf-1.outputs.cache-hit != 'true'
        uses: actions/checkout@v4
        with:
          repository: databricks/tpcds-kit
          ref: 2a5078a782192ddb6efbcead8de9973d6ab4f069
          path: ./tpcds-kit
      - name: Build tpcds-kit
        if: steps.cache-tpcds-sf-1.outputs.cache-hit != 'true'
        run: cd tpcds-kit/tools && make OS=LINUX
      - name: Generate TPC-DS (SF=1) table data
        if: steps.cache-tpcds-sf-1.outputs.cache-hit != 'true'
        run: build/sbt "sql/Test/runMain org.apache.spark.sql.GenTPCDSData --dsdgenDir `pwd`/tpcds-kit/tools --location `pwd`/tpcds-sf-1 --scaleFactor 1 --numPartitions 1 --overwrite"
      - name: Run TPC-DS queries (Sort merge join)
        run: |
          SPARK_TPCDS_DATA=`pwd`/tpcds-sf-1 build/sbt "sql/testOnly org.apache.spark.sql.TPCDSQueryTestSuite"
        env:
          SPARK_ANSI_SQL_MODE: ${{ fromJSON(inputs.envs).SPARK_ANSI_SQL_MODE }}
          SPARK_TPCDS_JOIN_CONF: |
            spark.sql.autoBroadcastJoinThreshold=-1
            spark.sql.join.preferSortMergeJoin=true
      - name: Run TPC-DS queries (Broadcast hash join)
        run: |
          SPARK_TPCDS_DATA=`pwd`/tpcds-sf-1 build/sbt "sql/testOnly org.apache.spark.sql.TPCDSQueryTestSuite"
        env:
          SPARK_ANSI_SQL_MODE: ${{ fromJSON(inputs.envs).SPARK_ANSI_SQL_MODE }}
          SPARK_TPCDS_JOIN_CONF: |
            spark.sql.autoBroadcastJoinThreshold=10485760
      - name: Run TPC-DS queries (Shuffled hash join)
        run: |
          SPARK_TPCDS_DATA=`pwd`/tpcds-sf-1 build/sbt "sql/testOnly org.apache.spark.sql.TPCDSQueryTestSuite"
        env:
          SPARK_ANSI_SQL_MODE: ${{ fromJSON(inputs.envs).SPARK_ANSI_SQL_MODE }}
          SPARK_TPCDS_JOIN_CONF: |
            spark.sql.autoBroadcastJoinThreshold=-1
            spark.sql.join.forceApplyShuffledHashJoin=true
      - name: Run TPC-DS queries on collated data
        run: |
          SPARK_TPCDS_DATA=`pwd`/tpcds-sf-1 build/sbt "sql/testOnly org.apache.spark.sql.TPCDSCollationQueryTestSuite"
      - name: Upload test results to report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-tpcds--${{ inputs.java }}-${{ inputs.hadoop }}-hive2.3
          path: "**/target/test-reports/*.xml"
      - name: Upload unit tests log files
        if: ${{ !success() }}
        uses: actions/upload-artifact@v4
        with:
          name: unit-tests-log-tpcds--${{ inputs.java }}-${{ inputs.hadoop }}-hive2.3
          path: "**/target/unit-tests.log"
