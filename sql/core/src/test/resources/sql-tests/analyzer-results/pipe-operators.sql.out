-- Automatically generated by SQLQueryTestSuite
-- !query
create table t(x int, y string) using csv
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false


-- !query
insert into t values (0, 'abc'), (1, 'def')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [x, y]
+- Project [cast(col1#x as int) AS x#x, cast(col2#x as string) AS y#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
table t
|> select 1 as x
-- !query analysis
Project [1 AS x#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x, y
-- !query analysis
Project [x#x, y#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x, y
|> select x + length(y) as z
-- !query analysis
Project [(x#x + length(y#x)) AS z#x]
+- Project [x#x, y#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
values (0), (1) tab(col)
|> select col * 2 as result
-- !query analysis
Project [(col#x * 2) AS result#x]
+- SubqueryAlias tab
   +- LocalRelation [col#x]


-- !query
(select * from t union all select * from t)
|> select x + length(y) as result
-- !query analysis
Project [(x#x + length(y#x)) AS result#x]
+- Union false, false
   :- Project [x#x, y#x]
   :  +- SubqueryAlias spark_catalog.default.t
   :     +- Relation spark_catalog.default.t[x#x,y#x] csv
   +- Project [x#x, y#x]
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select sum(x) as result
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_SELECT_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expr" : "sum(x#x) AS result#xL"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 34,
    "fragment" : "sum(x) as result"
  } ]
}


-- !query
drop table t
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t
