-- Automatically generated by SQLQueryTestSuite
-- !query
drop table if exists t
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t


-- !query
create table t(x int, y string) using csv
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false


-- !query
insert into t values (0, 'abc'), (1, 'def')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [x, y]
+- Project [cast(col1#x as int) AS x#x, cast(col2#x as string) AS y#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
drop table if exists other
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.other


-- !query
create table other(a int, b int) using json
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`other`, false


-- !query
insert into other values (1, 1), (1, 2), (2, 4)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/other, false, JSON, [path=file:[not included in comparison]/{warehouse_dir}/other], Append, `spark_catalog`.`default`.`other`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/other), [a, b]
+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
drop table if exists st
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.st


-- !query
create table st(x int, col struct<i1:int, i2:int>) using parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`st`, false


-- !query
insert into st values (1, (2, 3))
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/st, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/st], Append, `spark_catalog`.`default`.`st`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/st), [x, col]
+- Project [cast(col1#x as int) AS x#x, named_struct(i1, cast(col2#x.col1 as int), i2, cast(col2#x.col2 as int)) AS col#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
table t
|> select 1 as x
-- !query analysis
Project [1 AS x#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x, y
-- !query analysis
Project [x#x, y#x]
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x, y
|> select x + length(y) as z
-- !query analysis
Project [(x#x + length(y#x)) AS z#x]
+- Project [x#x, y#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
values (0), (1) tab(col)
|> select col * 2 as result
-- !query analysis
Project [(col#x * 2) AS result#x]
+- SubqueryAlias tab
   +- LocalRelation [col#x]


-- !query
(select * from t union all select * from t)
|> select x + length(y) as result
-- !query analysis
Project [(x#x + length(y#x)) AS result#x]
+- Union false, false
   :- Project [x#x, y#x]
   :  +- SubqueryAlias spark_catalog.default.t
   :     +- Relation spark_catalog.default.t[x#x,y#x] csv
   +- Project [x#x, y#x]
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(table t
 |> select x, y
 |> select x)
union all
select x from t where x < 1
-- !query analysis
Union false, false
:- Project [x#x]
:  +- Project [x#x, y#x]
:     +- SubqueryAlias spark_catalog.default.t
:        +- Relation spark_catalog.default.t[x#x,y#x] csv
+- Project [x#x]
   +- Filter (x#x < 1)
      +- SubqueryAlias spark_catalog.default.t
         +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
(select col from st)
|> select col.i1
-- !query analysis
Project [col#x.i1 AS i1#x]
+- Project [col#x]
   +- SubqueryAlias spark_catalog.default.st
      +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table st
|> select st.col.i1
-- !query analysis
Project [col#x.i1 AS i1#x]
+- SubqueryAlias spark_catalog.default.st
   +- Relation spark_catalog.default.st[x#x,col#x] parquet


-- !query
table t
|> select (select a from other where x = a limit 1)
-- !query analysis
Project [scalar-subquery#x [x#x] AS scalarsubquery(x)#x]
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Project [a#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select (select any_value(a) from other where x = a limit 1)
-- !query analysis
Project [scalar-subquery#x [x#x] AS scalarsubquery(x)#x]
:  +- GlobalLimit 1
:     +- LocalLimit 1
:        +- Aggregate [any_value(a#x, false) AS any_value(a)#x]
:           +- Filter (outer(x#x) = a#x)
:              +- SubqueryAlias spark_catalog.default.other
:                 +- Relation spark_catalog.default.other[a#x,b#x] json
+- SubqueryAlias spark_catalog.default.t
   +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select x + length(x) as z, z + 1 as plus_one
-- !query analysis
Project [z#x, (z#x + 1) AS plus_one#x]
+- Project [x#x, y#x, (x#x + length(cast(x#x as string))) AS z#x]
   +- SubqueryAlias spark_catalog.default.t
      +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select first_value(x) over (partition by y)
-- !query analysis
Project [first_value(x) OVER (PARTITION BY y ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x]
+- Project [x#x, y#x, first_value(x) OVER (PARTITION BY y ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x, first_value(x) OVER (PARTITION BY y ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x]
   +- Window [first_value(x#x, false) windowspecdefinition(y#x, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS first_value(x) OVER (PARTITION BY y ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#x], [y#x]
      +- Project [x#x, y#x]
         +- SubqueryAlias spark_catalog.default.t
            +- Relation spark_catalog.default.t[x#x,y#x] csv


-- !query
table t
|> select sum(x) as result
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_SELECT_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expr" : "sum(x#x) AS result#xL"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 34,
    "fragment" : "sum(x) as result"
  } ]
}


-- !query
table t
|> select y, length(y) + sum(x) as result
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_SELECT_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expr" : "y#x"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 19,
    "fragment" : "y"
  } ]
}


-- !query
drop table t
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t


-- !query
drop table other
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.other


-- !query
drop table st
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.st
