-- Automatically generated by SQLQueryTestSuite
-- !query
drop table if exists t
-- !query schema
struct<>
-- !query output



-- !query
create table t(x int, y string) using csv
-- !query schema
struct<>
-- !query output



-- !query
insert into t values (0, 'abc'), (1, 'def')
-- !query schema
struct<>
-- !query output



-- !query
drop table if exists other
-- !query schema
struct<>
-- !query output



-- !query
create table other(a int, b int) using json
-- !query schema
struct<>
-- !query output



-- !query
insert into other values (1, 1), (1, 2), (2, 4)
-- !query schema
struct<>
-- !query output



-- !query
drop table if exists st
-- !query schema
struct<>
-- !query output



-- !query
create table st(x int, col struct<i1:int, i2:int>) using parquet
-- !query schema
struct<>
-- !query output



-- !query
insert into st values (1, (2, 3))
-- !query schema
struct<>
-- !query output



-- !query
table t
|> select 1 as x
-- !query schema
struct<x:int>
-- !query output
1
1


-- !query
table t
|> select x, y
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> select x, y
|> select x + length(y) as z
-- !query schema
struct<z:int>
-- !query output
3
4


-- !query
values (0), (1) tab(col)
|> select col * 2 as result
-- !query schema
struct<result:int>
-- !query output
0
2


-- !query
(select * from t union all select * from t)
|> select x + length(y) as result
-- !query schema
struct<result:int>
-- !query output
3
3
4
4


-- !query
(table t
 |> select x, y
 |> select x)
union all
select x from t where x < 1
-- !query schema
struct<x:int>
-- !query output
0
0
1


-- !query
(select col from st)
|> select col.i1
-- !query schema
struct<i1:int>
-- !query output
2


-- !query
table st
|> select st.col.i1
-- !query schema
struct<i1:int>
-- !query output
2


-- !query
table t
|> select (select a from other where x = a limit 1) as result
-- !query schema
struct<result:int>
-- !query output
1
NULL


-- !query
select (values (0) tab(col) |> select col) as result
-- !query schema
struct<result:int>
-- !query output
0


-- !query
table t
|> select (select any_value(a) from other where x = a limit 1) as result
-- !query schema
struct<result:int>
-- !query output
1
NULL


-- !query
table t
|> select x + length(x) as z, z + 1 as plus_one
-- !query schema
struct<z:int,plus_one:int>
-- !query output
1	2
2	3


-- !query
table t
|> select first_value(x) over (partition by y) as result
-- !query schema
struct<result:int>
-- !query output
0
1


-- !query
select 1 x, 2 y, 3 z
|> select 1 + sum(x) over (),
     avg(y) over (),
     x,
     avg(x+1) over (partition by y order by z) AS a2
|> select a2
-- !query schema
struct<a2:double>
-- !query output
2.0


-- !query
table t
|> select x, count(*) over ()
|> select x
-- !query schema
struct<x:int>
-- !query output
0
1


-- !query
table t
|> select distinct x, y
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> select sum(x) as result
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_SELECT_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 24,
    "fragment" : "sum(x)"
  } ]
}


-- !query
table t
|> select y, length(y) + sum(x) as result
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "PIPE_OPERATOR_SELECT_CONTAINS_AGGREGATE_FUNCTION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "expr" : "sum(x#x)"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 34,
    "stopIndex" : 39,
    "fragment" : "sum(x)"
  } ]
}


-- !query
table t
|> tablesample (100 percent) repeatable (0)
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> tablesample (1 rows) repeatable (0)
-- !query schema
struct<x:int,y:string>
-- !query output
1	def


-- !query
table t
|> tablesample (bucket 1 out of 1) repeatable (0)
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> tablesample (100 percent) repeatable (0)
|> tablesample (5 rows) repeatable (0)
|> tablesample (bucket 1 out of 1) repeatable (0)
-- !query schema
struct<x:int,y:string>
-- !query output
0	abc
1	def


-- !query
table t
|> tablesample ()
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0014",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 25,
    "fragment" : "tablesample ()"
  } ]
}


-- !query
table t
|> tablesample (-100 percent)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0064",
  "messageParameters" : {
    "msg" : "Sampling fraction (-1.0) must be on interval [0, 1]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 37,
    "fragment" : "tablesample (-100 percent)"
  } ]
}


-- !query
table t
|> tablesample (-5 rows)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_LIMIT_LIKE_EXPRESSION.IS_NEGATIVE",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "expr" : "\"-5\"",
    "name" : "limit",
    "v" : "-5"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 25,
    "stopIndex" : 26,
    "fragment" : "-5"
  } ]
}


-- !query
table t
|> tablesample (x rows)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "INVALID_LIMIT_LIKE_EXPRESSION.IS_UNFOLDABLE",
  "sqlState" : "42K0E",
  "messageParameters" : {
    "expr" : "\"x\"",
    "name" : "limit"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 25,
    "stopIndex" : 25,
    "fragment" : "x"
  } ]
}


-- !query
table t
|> tablesample (bucket 2 out of 1)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0064",
  "messageParameters" : {
    "msg" : "Sampling fraction (2.0) must be on interval [0, 1]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 42,
    "fragment" : "tablesample (bucket 2 out of 1)"
  } ]
}


-- !query
table t
|> tablesample (200b) repeatable (0)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0015",
  "messageParameters" : {
    "msg" : "byteLengthLiteral"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 44,
    "fragment" : "tablesample (200b) repeatable (0)"
  } ]
}


-- !query
table t
|> tablesample (200) repeatable (0)
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0016",
  "messageParameters" : {
    "bytesStr" : "200"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 12,
    "stopIndex" : 43,
    "fragment" : "tablesample (200) repeatable (0)"
  } ]
}


-- !query
drop table t
-- !query schema
struct<>
-- !query output



-- !query
drop table other
-- !query schema
struct<>
-- !query output



-- !query
drop table st
-- !query schema
struct<>
-- !query output

